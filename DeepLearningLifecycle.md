<div class="css-1apu423"><div class="css-u8svcc"><div class="copy-length css-0"><div class="ureact-markdown css-tc5hjw"><h2 class="chakra-heading css-fz7yxd">Deep Learning Lifecycle:</h2>
<p class="chakra-text css-o3oz8b">We begin with a problem statement, then move to development (where code gets written). Next, we begin training, where our model learns our data. After training, we deploy, which is when our model goes out into the world for use. A lot of other people do front-end work at this stage. Then we go to monitoring. For our purposes, we'll focus on the <strong>development</strong> and <strong>training</strong> of models.</p>
<h2 class="chakra-heading css-fz7yxd">Deep Learning Tools</h2>
<h3 class="chakra-heading css-k57syw">Development tools</h3>
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Integrated Development Environment
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Code Editor</li><li class="css-cvpopp">Interpreter/Compiler</li></ul>
</li><li class="css-cvpopp">Jupyter Notebooks
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Note: Each cell is executed on its own, and it works well for environments to prototype or present code. However, there are limitations:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Editing can make you lose state</li><li class="css-cvpopp">Code deployed production should be in .py rather than notebooks</li></ul>
</li></ul>
</li></ul>
<h3 class="chakra-heading css-k57syw">Deep Learning Frameworks</h3>
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">PyTorch (aka Torch)</li><li class="css-cvpopp">TensorFlow/Keras</li><li class="css-cvpopp">JAX</li></ul>
<h3 class="chakra-heading css-k57syw">Training Tools</h3>
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Experiment management like TensorBoard or Weights and Biases
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Observe accuracy and loss at training time</li></ul>
</li><li class="css-cvpopp">Model versioning like DVC, Neptune, and Pachyderm
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Remedy issues within the model across different versions of the model</li><li class="css-cvpopp">DVC is very similar to Git</li></ul>
</li></ul></div></div></div></div>


<div class="ureact-markdown css-tc5hjw"><h2 class="chakra-heading css-fz7yxd">Deciding when Deep Learning is the Right Tool</h2>
<p class="chakra-text css-o3oz8b">Deep Learning is a powerful tool, but it's not the only one. In general, the way to choose whether or not to use Deep Learning depends on your task, what kind of data you have, and how complex the relationships in the data are.</p>
<p class="chakra-text css-o3oz8b">Scenarios in which to use Deep Learning include but are not limited to:</p>
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp"><strong>Tasks</strong>
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Binary classification:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Deep Learning</li><li class="css-cvpopp">Logistic Regression</li><li class="css-cvpopp">Decision Trees</li><li class="css-cvpopp">Support Vector Machines</li></ul>
</li><li class="css-cvpopp">Multi-class classification:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Deep Learning</li><li class="css-cvpopp">Decision Trees</li><li class="css-cvpopp">Support Vector Machines</li></ul>
</li><li class="css-cvpopp">Regression:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Deep Learning</li><li class="css-cvpopp">Linear Regression</li><li class="css-cvpopp">some Decision Trees</li></ul>
</li></ul>
</li><li class="css-cvpopp"><strong>Data:</strong>
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Images:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Deep Learning</li></ul>
</li><li class="css-cvpopp">Text:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Deep Learning</li><li class="css-cvpopp">Statistical Methods</li></ul>
</li><li class="css-cvpopp">Tabular Data:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Decision Trees</li></ul>
</li></ul>
</li><li class="css-cvpopp"><strong>Data Relationships</strong>:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Simple:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Linear Regression</li><li class="css-cvpopp">Logistic Regression</li><li class="css-cvpopp">Decision Trees</li></ul>
</li><li class="css-cvpopp">Complex:
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">Deep Learning</li><li class="css-cvpopp">Decision Trees</li></ul>
</li></ul>
</li></ul></div>

<div class="ureact-markdown css-tc5hjw"><h2 class="chakra-heading css-fz7yxd">Computer Vision Object Detection Model</h2>
<p class="chakra-text css-o3oz8b">Deep learning is used for myriad applications, from sentiment analysis (Natural Language Processing) and image recognition (Computer Vision) to the generation of text and images (Multimodal).</p>
<p class="chakra-text css-o3oz8b"><a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://arxiv.org/abs/1506.02640">You Only Look Once, or YOLO<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>, is an <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://en.wikipedia.org/wiki/Object_detection">object recognition<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> model still very effective today, years after its release. The model is based on a <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks">Convolutional Neural Network<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> architecture, i.e., a specialized deep learning architecture primarily used in Computer Vision.</p>
<p class="chakra-text css-o3oz8b">YOLO uses anchor boxes to predict the coordinates of the bounding boxes around objects. It also applies non-maximum suppression to remove duplicate detections and refine the bounding box locations. This model is known for its speed and accuracy, making it a popular choice for real-time object detection applications, such as self-driving cars, surveillance systems, and robotics.</p>
<p class="chakra-text css-o3oz8b">At a high level, YOLO and other convolutional neural networks take images of 3-dimensional tensors -- you can think of a Rubik's cube as a 3 x 3 x 3 tensor -- that uses a mathematical operation called convolution to generate representations of different properties of an image. These properties are passed on to further layers that identify other properties of an image and, eventually, output a prediction about the image.</p>
<h2 class="chakra-heading css-fz7yxd">TinyYOLOv2 - YOLO Lightweight Model</h2>
<p class="chakra-text css-o3oz8b">Carnegie Mellon University has a <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://www.cs.cmu.edu/~dst/FaceDemo/index.html">TinyYOLOv2 implementation<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> available for experimentation that explores pre-loaded images and allows new images to be uploaded. TinyYOLO is much faster but less accurate than the standard YOLO model.</p>
<p class="chakra-text css-o3oz8b">The TinyYOLOv2 model is a real-time neural network that detects 20 different classes and comprises nine convolutional layers and six max-pooling layers. This model is lightweight and designed to be computationally efficient, making it a good choice for applications with limited computing resources, such as live webcams, mobile devices, or embedded systems.</p></div>


<div class="css-1rr4qq7"><div class="css-dvxtzn"><div class="css-1apu423"><div class="css-u8svcc"><div class="copy-length css-0"><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b"><a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://openai.com/research/dall-e">DALL-E<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> is a project from OpenAI designed to generate images from text. There are many other systems now for generating these images, such as <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://www.midjourney.com/">Midjourney<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> and <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://huggingface.co/spaces/stabilityai/stable-diffusion">Stable Diffusion<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>. These image generation models are a class of physics-inspired <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://en.wikipedia.org/wiki/Diffusion_model">Diffusion Model<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>.</p>
<p class="chakra-text css-o3oz8b"><a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://github.com/borisdayma/dalle-mini">DALL-E mini<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> is a smaller version of the DALL·E model designed to run on consumer-grade equipment but still offers excellent performance!</p>
<blockquote class="css-6jmydm">
<p class="chakra-text css-o3oz8b">If you are interested in learning more about DALL·E Mini dataset, architecture, and model training, you can read this article by the authors: <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA">DALL·E Mini Explained<span class="chakra-text css-1lktits">(opens in a new tab)</span></a></p>
</blockquote>
<h2 class="chakra-heading css-fz7yxd">Exercise</h2>
<p class="chakra-text css-o3oz8b">In this exercise, you can experiment with creating images from texts using the DALL-E Mini model in two ways:</p>
<ol role="list" class="css-13a5a39"><li class="css-cvpopp"><a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://huggingface.co/spaces/dalle-mini/dalle-mini">No-code DALL·E Mini demo<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> by <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://www.craiyon.com/">craiyon.com<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> on HuggingFace Space</li><li class="css-cvpopp">Run DALL·E Mini inference pipeline on Jupyter Notebook <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://github.com/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb">[GitHub link]<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://colab.research.google.com/github/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb">[Google Colab link]<span class="chakra-text css-1lktits">(opens in a new tab)</span></a></li></ol>
<p class="chakra-text css-o3oz8b"><em class="chakra-text css-o3oz8b">Note</em>: To run this in Google Colab, you would need an A100 instance, which is only available with a paid subscription</p>
<br>
<p class="chakra-text css-o3oz8b"><em class="chakra-text css-o3oz8b"><strong>Notes on DALL·E Mini Inference Pipeline Notebook:</strong></em></p>
<ol role="list" class="css-13a5a39"><li class="css-cvpopp">Before you run the Jupyter Notebook, you need to get an API key from <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://wandb.ai/">wandb.ai<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>.
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">You can create a new account and find the API key here: <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://wandb.ai/authorize">https://wandb.ai/authorize<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>.</li></ul>
</li><li class="css-cvpopp">The notebook requires <code class="chakra-code css-cn4kbm">jax</code> and <code class="chakra-code css-cn4kbm">jaxlib</code> libraries v0.3.25.
<ul role="list" class="css-19qh3zo"><li class="css-cvpopp">After you run the first cell to pip install <code class="chakra-code css-cn4kbm">dalle-mini</code> and <code class="chakra-code css-cn4kbm">vggan-jax</code>, create a new cell, and install <code class="chakra-code css-cn4kbm">jax</code> and <code class="chakra-code css-cn4kbm">jaxlib</code> v0.3.25</li></ul>
</li></ol>
<blockquote class="css-6jmydm">
<p class="chakra-text css-o3oz8b">!pip install jax==0.3.25<br>
!pip install jaxlib==0.3.25</p>
</blockquote>
<p class="chakra-text css-o3oz8b">Right-click and save DALL·E Mini Inference Pipeline Jupyter Notebook with the correct jax library <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://video.udacity-data.com/topher/2023/March/641ba138_dalle_mini_inference_pipeline/dalle_mini_inference_pipeline.ipynb">here<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>.</p></div></div></div></div><div class="css-t0opw2"><div class="css-u8svcc"><div role="group" aria-labelledby="title-18474537" class="css-1do9afp"><div class="css-k008qs"><img alt="" src="/assets/images/icons/light-paper-checklist.svg" class="chakra-image css-1cph6iq"><div><label id="title-18474537" class="chakra-form__label chakra-heading css-asihqn">Instructions</label><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b">Running DALL-E mini on your own machine</p>
<p class="chakra-text css-o3oz8b">We recommend following along with the <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://github.com/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb">guided example<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> from the DALL-E mini Github</p></div></div></div><div class="chakra-stack css-1ac1sc7"><label class="chakra-checkbox css-lcxgrr"><input class="chakra-checkbox__input" type="checkbox" id="18474537--0" value="" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; width: 1px; margin: -1px; padding: 0px; overflow: hidden; white-space: nowrap; position: absolute;"><span class="chakra-checkbox__control css-1cbh079" aria-hidden="true"></span><span class="chakra-checkbox__label css-1sgc0qu"><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b">Install DALL-E mini on your machine by running <code class="chakra-code css-cn4kbm">pip install dalle-mini</code></p></div></span></label><label class="chakra-checkbox css-lcxgrr"><input class="chakra-checkbox__input" type="checkbox" id="18474537--1" value="" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; width: 1px; margin: -1px; padding: 0px; overflow: hidden; white-space: nowrap; position: absolute;"><span class="chakra-checkbox__control css-1cbh079" aria-hidden="true"></span><span class="chakra-checkbox__label css-1sgc0qu"><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b">Either on your own or using the <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://github.com/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb">guided example notebook<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>, load up a Jupyter notebook and import <code class="chakra-code css-cn4kbm">dalle-mini</code></p></div></span></label><label class="chakra-checkbox css-lcxgrr"><input class="chakra-checkbox__input" type="checkbox" id="18474537--2" value="" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; width: 1px; margin: -1px; padding: 0px; overflow: hidden; white-space: nowrap; position: absolute;"><span class="chakra-checkbox__control css-1cbh079" aria-hidden="true"></span><span class="chakra-checkbox__label css-1sgc0qu"><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b">Using <code class="chakra-code css-cn4kbm">DalleBart</code> and <code class="chakra-code css-cn4kbm">DalleBartProcessor</code>, load a checkpoint for inference.</p></div></span></label><label class="chakra-checkbox css-lcxgrr"><input class="chakra-checkbox__input" type="checkbox" id="18474537--3" value="" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; width: 1px; margin: -1px; padding: 0px; overflow: hidden; white-space: nowrap; position: absolute;"><span class="chakra-checkbox__control css-1cbh079" aria-hidden="true"></span><span class="chakra-checkbox__label css-1sgc0qu"><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b">Write a prompt and tokenize it with the <code class="chakra-code css-cn4kbm">DalleBartProcessor</code></p></div></span></label><label class="chakra-checkbox css-lcxgrr"><input class="chakra-checkbox__input" type="checkbox" id="18474537--4" value="" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; width: 1px; margin: -1px; padding: 0px; overflow: hidden; white-space: nowrap; position: absolute;"><span class="chakra-checkbox__control css-1cbh079" aria-hidden="true"></span><span class="chakra-checkbox__label css-1sgc0qu"><div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b">Generate examples from your prompt using the <code class="chakra-code css-cn4kbm">.generate()</code> method of your <code class="chakra-code css-cn4kbm">DalleBart</code> model</p></div></span></label></div></div></div></div></div></div>


<div class="ureact-markdown css-tc5hjw"><p class="chakra-text css-o3oz8b"><a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://github.com/openai/whisper">Whisper<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> is a speech-recognition model by OpenAI designed to allow for language identification, translation, and speech recognition.  As covered in the <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://openai.com/research/whisper">Whisper blog post<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>, this model approaches human-level English speech recognition -- the sort of technology that powers digital assistants and modern speech-to-text for sending text messages!</p>
<p class="chakra-text css-o3oz8b">As shown below, Whisper's architecture is known as a "sequence to sequence" or <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://en.wikipedia.org/wiki/Seq2seq">seq2seq model<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> widely used in machine translation. The model is based on the <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer architecture<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> and undergoes training for multiple speech processing tasks, such as multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. The model employs a decoder to predict a sequence of tokens collectively representing these tasks, allowing a single model to replace many stages of a <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://www.researchgate.net/figure/Traditional-feature-extraction-pipeline-for-speech-processing-The-spectral-analysis_fig10_338935292">traditional speech-processing pipeline<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>.</p>
<p class="chakra-text css-o3oz8b">In this demo, you can test out the power of Whisper for yourself! You can clone the <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://github.com/openai/whisper">GitHub repo<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> and work through the examples or use their provided <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb">Google Colab<span class="chakra-text css-1lktits">(opens in a new tab)</span></a> example. Using a base Whisper model, the notebook runs inference on the <a target="_blank" rel="noopener noreferrer" class="chakra-link css-1b272on" href="http://www.danielpovey.com/files/2015_icassp_librispeech.pdf">LibriSpeech ASR Corpus<span class="chakra-text css-1lktits">(opens in a new tab)</span></a>, a popular benchmark dataset for evaluating speech recognition models.</p></div>

![image](https://github.com/HemanthSaiTejaLingam/StudyMaterials/assets/114983155/9b942909-e129-4f8b-ab01-1d54e7ccc727)
